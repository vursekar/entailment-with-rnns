{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy, pandas, tensorflow, and other packages\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a path to snli data from current directory to create Pandas dataframe\n",
    "path_to_training_data = '../snli_1.0/' \n",
    "ds = pd.read_csv(path_to_training_data+'snli_1.0_train.txt',delimiter='\\t')\n",
    "ds = ds[ds['gold_label']!='-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data by removing non-alphanumeric characters and tokenizing \n",
    "def preprocessSentence(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
    "    s = s.strip()\n",
    "    #s = '<start> ' + s + ' <end>'\n",
    "    return s\n",
    "\n",
    "\n",
    "# Create a TF tokenizer that creates a word index\n",
    "def createTokenizer(data):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    return tokenizer\n",
    "\n",
    "def tokenizeData(data,tokenizer):\n",
    "    tensor = tokenizer.texts_to_sequences(data)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "    return tensor\n",
    "\n",
    "premises = []\n",
    "for premise in ds['sentence1'].astype('str'):\n",
    "    premises.append(preprocessSentence(premise))\n",
    "\n",
    "hypotheses = []\n",
    "for hypothesis in ds['sentence2'].astype('str'):\n",
    "    hypotheses.append(preprocessSentence(hypothesis))\n",
    "    \n",
    "tokenizer = createTokenizer(premises+hypotheses)\n",
    "\n",
    "# Now have buffer x seq. length tensor, where seq. length is the maximum length of a sentence in the set\n",
    "premise_tensor = tokenizeData(premises,tokenizer)\n",
    "hypothesis_tensor = tokenizeData(hypotheses,tokenizer)\n",
    "\n",
    "# Concatenate premises and hypotheses so that they can be fed as one tensor\n",
    "ph_tensor = tf.concat([premise_tensor,hypothesis_tensor],1)\n",
    "ph_labels = ds['gold_label'].map({'neutral':0,'contradiction':1,'entailment':2}).astype('int').values\n",
    "split_size = premise_tensor.shape[1]\n",
    "\n",
    "del ds\n",
    "del premises\n",
    "del hypotheses\n",
    "del premise_tensor\n",
    "del hypothesis_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters and other info\n",
    "embedding_dim = 100\n",
    "BUFFER_SIZE = len(ph_tensor)\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
    "units = 100\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "train_input, val_input, train_targ, val_targ = train_test_split(ph_tensor.numpy(),\n",
    "                                                                ph_labels,\n",
    "                                                                test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# To use GLOVE VECTORS uncomment\n",
    "GLOVE_DIR = '../glove/'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tf data pipeline\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_input, train_targ)) #.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_input,val_targ)) #.shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoder + Decoder architectures\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,vocab_size,embedding_dim,enc_units,batch_size):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        \n",
    "        \n",
    "        # Uncomment first and comment second lines to use random initial word embeddings\n",
    "        #self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim,mask_zero=True)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim,\n",
    "                                                   weights=[embedding_matrix],\n",
    "                                                   trainable=False,\n",
    "                                                   mask_zero=True)\n",
    "\n",
    "        \n",
    "        \n",
    "#         self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "#                                    return_sequences=True,\n",
    "#                                    return_state=True,\n",
    "#                                    recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   dropout=0.05)\n",
    "        \n",
    "        \n",
    "    def call(self,x,hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        #output, state = self.gru(x,initial_state=hidden)\n",
    "        #return output, state\n",
    "        output, state, carry = self.lstm(x,initial_state=hidden)\n",
    "        return output, state, carry\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size,self.enc_units))\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,units):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.W3 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self,query,past_context,values):\n",
    "        query_with_time_axis = tf.expand_dims(query,1)\n",
    "        past_context_with_time = tf.expand_dims(past_context,1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values) + self.W3(past_context_with_time)))\n",
    "        attention_weights = tf.nn.softmax(score,axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "\n",
    "# Attention Layer\n",
    "class BahdanauAttentionLastWord(tf.keras.layers.Layer):\n",
    "    def __init__(self,units):\n",
    "        super(BahdanauAttentionLastWord,self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self,query,values):\n",
    "        query_with_time_axis = tf.expand_dims(query,1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score,axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        \n",
    "        # Uncomment first and comment second lines to use random initial word embeddings\n",
    "        \n",
    "        #self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,mask_zero=True)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim,\n",
    "                                                   weights=[embedding_matrix],\n",
    "                                                   trainable=False,mask_zero=True)\n",
    "        \n",
    "#         self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "#                                        return_sequences=True,\n",
    "#                                        return_state=True,\n",
    "#                                        recurrent_initializer='glorot_uniform')\n",
    "        self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout=0.05)\n",
    "        #self.attention = BahdanauAttention(self.dec_units)\n",
    "        self.attention = BahdanauAttentionLastWord(self.dec_units)\n",
    "        self.Wp = tf.keras.layers.Dense(units,activation='linear')\n",
    "        self.Wx = tf.keras.layers.Dense(units,activation='linear')\n",
    "        self.fc = tf.keras.layers.Dense(3,kernel_regularizer=regularizers.l2(0.001))\n",
    "        \n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        #output, state = self.gru(x,initial_state=hidden)\n",
    "        output, state, carry = self.lstm(x,initial_state=hidden)\n",
    "        \n",
    "#         past_context_vector = hidden\n",
    "#         for t in range(x.shape[1]):\n",
    "#             context_vector, _ = self.attention(output[:,t,:], past_context_vector,enc_output)\n",
    "#             past_context_vector = context_vector\n",
    "\n",
    "        context_vector, _ = self.attention(state,enc_output)\n",
    "    \n",
    "        context_vector = tf.nn.tanh(self.Wp(context_vector) + self.Wx(state))\n",
    "    \n",
    "        #context_vector = tf.concat([context_vector,state],axis=1)\n",
    "        \n",
    "        x = self.fc(context_vector)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size,self.dec_units))\n",
    "    \n",
    "    \n",
    "class RTEAttention(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_dim,enc_units,dec_units,batch_size,split_size, **kwargs):\n",
    "        super(RTEAttention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.split_size = split_size\n",
    "        self.encoder = Encoder(vocab_size,embedding_dim,enc_units,batch_size)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, dec_units, batch_size)\n",
    "        \n",
    "    def call(self,inp):\n",
    "        enc_hidden = self.encoder.initialize_hidden_state()\n",
    "        premise, hypothesis = tf.split(inp,[self.split_size,-1],axis=1)\n",
    "        #enc_output, enc_hidden = self.encoder(premise,enc_hidden)\n",
    "        #predictions = self.decoder(hypothesis, enc_hidden, enc_output)\n",
    "        enc_output, enc_hidden, enc_cell = self.encoder(premise)\n",
    "        \n",
    "        dec_hidden = self.decoder.initialize_hidden_state()\n",
    "        #predictions = self.decoder(hypothesis, [enc_hidden, enc_cell], enc_output)\n",
    "        predictions = self.decoder(hypothesis, [dec_hidden, enc_cell], enc_output)\n",
    "         \n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = RTEAttention(vocab_size,embedding_dim,units,units,BATCH_SIZE,split_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters and compile\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer, \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3862/3862 [==============================] - 1837s 476ms/step - loss: 0.6548 - accuracy: 0.7244 - val_loss: 0.6515 - val_accuracy: 0.7264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x139672f70>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit on training data\n",
    "model.fit(train_dataset, epochs=1,validation_data=val_dataset,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./checkpoints/snli17102020_k100_glove_dropout05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
