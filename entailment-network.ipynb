{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "ds = pd.read_csv('train.csv')\n",
    "ds = ds[ds['language']=='English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data by removing non-alphanumeric characters and tokenizing \n",
    "def preprocessSentence(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
    "    s = s.strip()\n",
    "    s = '<start> ' + s + ' <end>'\n",
    "    return s\n",
    "\n",
    "def createTokenizer(data):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    return tokenizer\n",
    "\n",
    "def tokenizeData(data,tokenizer):\n",
    "    tensor = tokenizer.texts_to_sequences(data)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "    return tensor\n",
    "\n",
    "premises = []\n",
    "for premise in ds['premise']:\n",
    "    premises.append(preprocessSentence(premise))\n",
    "\n",
    "hypotheses = []\n",
    "for hypothesis in ds['hypothesis']:\n",
    "    hypotheses.append(preprocessSentence(hypothesis))\n",
    "    \n",
    "tokenizer = createTokenizer(premises+hypotheses)\n",
    "premise_tensor = tokenizeData(premises,tokenizer)\n",
    "hypothesis_tensor = tokenizeData(hypotheses,tokenizer)\n",
    "ph_tensor = tf.concat([premise_tensor,hypothesis_tensor],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf data pipeline\n",
    "BUFFER_SIZE = len(ph_tensor)\n",
    "BATCH_SIZE = 32\n",
    "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 200\n",
    "units = 150\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "split_size = premise_tensor.shape[1]\n",
    "\n",
    "train_input, val_input, train_targ, val_targ = train_test_split(ph_tensor.numpy(),ds['label'].values,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_input, train_targ)) #.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_input,val_targ)) #.shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoder + Decoder architectures\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,vocab_size,embedding_dim,enc_units,batch_size):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
    "#         self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "#                                    return_sequences=True,\n",
    "#                                    return_state=True,\n",
    "#                                    recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        \n",
    "    def call(self,x,hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        #output, state = self.gru(x,initial_state=hidden)\n",
    "        #return output, state\n",
    "        output, state, carry = self.lstm(x,initial_state=hidden)\n",
    "        return output, state, carry\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size,self.enc_units))\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,units):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.W3 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self,query,past_context,values):\n",
    "        query_with_time_axis = tf.expand_dims(query,1)\n",
    "        past_context_with_time = tf.expand_dims(past_context,1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values) + self.W3(past_context_with_time)))\n",
    "        attention_weights = tf.nn.softmax(score,axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "class BahdanauAttentionLastWord(tf.keras.layers.Layer):\n",
    "    def __init__(self,units):\n",
    "        super(BahdanauAttentionLastWord,self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self,query,values):\n",
    "        query_with_time_axis = tf.expand_dims(query,1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score,axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "#         self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "#                                        return_sequences=True,\n",
    "#                                        return_state=True,\n",
    "#                                        recurrent_initializer='glorot_uniform')\n",
    "        self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        #self.attention = BahdanauAttention(self.dec_units)\n",
    "        self.attention = BahdanauAttentionLastWord(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(3)\n",
    "        \n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        #output, state = self.gru(x,initial_state=hidden)\n",
    "        output, state, carry = self.lstm(x,initial_state=hidden)\n",
    "        \n",
    "#         past_context_vector = hidden\n",
    "#         for t in range(x.shape[1]):\n",
    "#             context_vector, _ = self.attention(output[:,t,:], past_context_vector,enc_output)\n",
    "#             past_context_vector = context_vector\n",
    "\n",
    "        context_vector, _ = self.attention(state,enc_output)\n",
    "    \n",
    "        context_vector = tf.concat([context_vector,state],axis=1)\n",
    "        \n",
    "        x = self.fc(context_vector)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class RTEAttention(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_dim,enc_units,dec_units,batch_size,split_size, **kwargs):\n",
    "        super(RTEAttention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.split_size = split_size\n",
    "        self.encoder = Encoder(vocab_size,embedding_dim,enc_units,batch_size)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, dec_units, batch_size)\n",
    "        \n",
    "    def call(self,inp):\n",
    "        enc_hidden = self.encoder.initialize_hidden_state()\n",
    "        premise, hypothesis = tf.split(inp,[self.split_size,-1],axis=1)\n",
    "        #enc_output, enc_hidden = self.encoder(premise,enc_hidden)\n",
    "        #predictions = self.decoder(hypothesis, enc_hidden, enc_output)\n",
    "        enc_output, enc_hidden, enc_cell = self.encoder(premise)\n",
    "        predictions = self.decoder(hypothesis, [enc_hidden, enc_cell], enc_output)\n",
    "         \n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 3), dtype=float32, numpy=\n",
       "array([[ 0.09046682, -0.04434597,  0.00650291],\n",
       "       [ 0.07976421, -0.04252859,  0.00456191],\n",
       "       [ 0.0865448 , -0.04405873,  0.0060536 ],\n",
       "       [ 0.09041405, -0.04534531,  0.00661955],\n",
       "       [ 0.08427687, -0.04285638,  0.00555488],\n",
       "       [ 0.09178941, -0.04525625,  0.00720036],\n",
       "       [ 0.08985469, -0.04468172,  0.00675021],\n",
       "       [ 0.08949722, -0.04464971,  0.00677291],\n",
       "       [ 0.08559003, -0.04375186,  0.00570524],\n",
       "       [ 0.08745772, -0.04428917,  0.00637429],\n",
       "       [ 0.08377945, -0.04301215,  0.00656908],\n",
       "       [ 0.08944805, -0.04442884,  0.00655987],\n",
       "       [ 0.09086208, -0.04462938,  0.00673357],\n",
       "       [ 0.08803997, -0.04393613,  0.00592776],\n",
       "       [ 0.09256784, -0.04616264,  0.00721948],\n",
       "       [ 0.0908728 , -0.04504931,  0.00654482],\n",
       "       [ 0.08438527, -0.0431409 ,  0.0056338 ],\n",
       "       [ 0.09238025, -0.04559826,  0.00696244],\n",
       "       [ 0.08716755, -0.0438043 ,  0.00611146],\n",
       "       [ 0.08612444, -0.04422703,  0.00647624],\n",
       "       [ 0.08678199, -0.04439713,  0.00628587],\n",
       "       [ 0.09000567, -0.04438335,  0.00633139],\n",
       "       [ 0.07818449, -0.04057094,  0.0042198 ],\n",
       "       [ 0.08188214, -0.04321919,  0.00465616],\n",
       "       [ 0.08506093, -0.04275365,  0.00581269],\n",
       "       [ 0.0802496 , -0.0405487 ,  0.00468852],\n",
       "       [ 0.09110724, -0.04531987,  0.00668274],\n",
       "       [ 0.0932579 , -0.04599089,  0.00735556],\n",
       "       [ 0.08544182, -0.04486062,  0.00641186],\n",
       "       [ 0.08711872, -0.04293709,  0.00549872],\n",
       "       [ 0.09128264, -0.04474182,  0.00692617],\n",
       "       [ 0.08968706, -0.04483583,  0.00655374]], dtype=float32)>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = next(iter(train_dataset))\n",
    "model(sample_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RTEAttention(vocab_size,embedding_dim,units,units,BATCH_SIZE,split_size)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer, \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 19/150 [==>...........................] - ETA: 1:02 - loss: 1.0993 - accuracy: 0.3438"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=5,validation_data=val_dataset,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
